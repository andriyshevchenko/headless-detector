name: E2E Tests

on:
  pull_request:
    branches:
      - '**'
  workflow_dispatch:

# Prevent cancellation of in-progress runs when new commits are pushed
# Each run gets its own unique group to avoid cancellation
concurrency:
  group: e2e-${{ github.run_id }}-${{ github.run_attempt }}
  cancel-in-progress: false

# Run tests in parallel using matrix strategy
# Each test runs in its own isolated job (~7 min each including setup)
# Total CI time: ~8 min instead of ~40 min sequential

jobs:
  e2e-test:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # 5 min test + 10 min buffer for setup/teardown/retries
    permissions:
      contents: read
    
    strategy:
      fail-fast: false  # Continue other tests if one fails
      matrix:
        test:
          # Level 1-3: BOT classification (expected score ≥0.40)
          - name: L1-naive-robot
            grep: "5-minute L1-naive-robot behavior"
          - name: L3-impulsive-robot
            grep: "5-minute L3-impulsive-robot behavior"
          # Level 4: SUSPICIOUS classification (expected score 0.30-0.40)
          - name: L4-burst-pattern
            grep: "5-minute L4-burst-pattern behavior"
          - name: L4-replay-pattern
            grep: "5-minute L4-replay-pattern behavior"
          # Level 5: SUSPICIOUS classification (expected score 0.30-0.40)
          - name: L5-interleaved-actions
            grep: "5-minute L5-interleaved-actions behavior"
          - name: L5-scroll-focused
            grep: "5-minute L5-scroll-focused behavior"
          - name: L5-keyboard-focused
            grep: "5-minute L5-keyboard-focused behavior"
          - name: L5-gaussian-timing
            grep: "5-minute L5-gaussian-timing behavior"
          - name: L5-mixed-behaviors
            grep: "5-minute L5-mixed-behaviors"
          # Level 6: SUSPICIOUS classification (expected score 0.30-0.40)
          - name: L6-bezier-with-noise
            grep: "5-minute L6-bezier-with-noise behavior"
          - name: L6-mouse-focused
            grep: "5-minute L6-mouse-focused behavior"
          # Level 7: LIKELY_HUMAN classification (expected score 0.15-0.30)
          - name: L7-fast-bezier
            grep: "5-minute L7-fast-bezier behavior"
          - name: L7-impulsive-bezier
            grep: "5-minute L7-impulsive-bezier behavior"
          - name: L7-slow-bezier
            grep: "5-minute L7-slow-bezier behavior"
          - name: L7-phase-alternating
            grep: "5-minute L7-phase-alternating behavior"
          # Level 8: LIKELY_HUMAN classification (expected score 0.15-0.30)
          - name: L8-full-human-sim
            grep: "5-minute L8-full-human-sim behavior"
          - name: L8-smooth-bezier
            grep: "5-minute L8-smooth-bezier behavior"
          # Level 9-10: VERIFIED classification (expected score <0.15)
          - name: L9-advanced-human
            grep: "5-minute L9-advanced-human behavior"
          - name: L10-ultimate-evasion
            grep: "5-minute L10-ultimate-evasion behavior"
    
    name: E2E - ${{ matrix.test.name }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20.x'
      
      - name: Install dependencies
        run: npm install
      
      - name: Install Playwright browsers
        run: npx playwright install --with-deps chromium
      
      - name: Run ${{ matrix.test.name }} test
        id: run_test
        run: |
          # Run test and capture output
          npx playwright test --grep "${{ matrix.test.grep }}" 2>&1 | tee test-output.log
          
          # Extract calibration JSON from output for artifact storage
          mkdir -p calibration-data
          grep -A 10000 "===CALIBRATION_JSON_START===" test-output.log | grep -B 10000 "===CALIBRATION_JSON_END===" | grep -v "===CALIBRATION" > calibration-data/${{ matrix.test.name }}-calibration.json || echo '{}' > calibration-data/${{ matrix.test.name }}-calibration.json
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-report-${{ matrix.test.name }}
          path: playwright-report/
          retention-days: 7
      
      - name: Upload test traces
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: playwright-traces-${{ matrix.test.name }}
          path: test-results/
          retention-days: 7
      
      # Upload calibration data as artifact for metric analysis
      - name: Upload calibration data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: calibration-${{ matrix.test.name }}
          path: calibration-data/
          retention-days: 30  # Keep metrics longer for analysis

  # Summary job that waits for all tests and reports overall status
  # Also aggregates all calibration data into a single artifact
  e2e-summary:
    needs: e2e-test
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Download all calibration data
        uses: actions/download-artifact@v4
        with:
          pattern: calibration-*
          path: all-calibration-data
          merge-multiple: true
      
      - name: Aggregate calibration data
        run: |
          echo "=== Aggregating calibration data from all tests ==="
          mkdir -p aggregated-metrics
          
          # Create a combined JSON file with all metrics
          echo '{"tests": [' > aggregated-metrics/all-metrics.json
          first=true
          for f in all-calibration-data/*.json; do
            if [ -f "$f" ] && [ -s "$f" ]; then
              if [ "$first" = true ]; then
                first=false
              else
                echo ',' >> aggregated-metrics/all-metrics.json
              fi
              cat "$f" >> aggregated-metrics/all-metrics.json
            fi
          done
          echo ']}' >> aggregated-metrics/all-metrics.json
          
          # Show summary
          echo "=== Calibration data summary ==="
          cat aggregated-metrics/all-metrics.json | head -100
          echo "..."
          echo "Full data saved to aggregated-metrics/all-metrics.json"
      
      # Upload aggregated metrics - this is the main artifact for analysis
      - name: Upload aggregated metrics
        uses: actions/upload-artifact@v4
        with:
          name: all-calibration-metrics
          path: aggregated-metrics/
          retention-days: 30
      
      - name: Check test results
        run: |
          if [ "${{ needs.e2e-test.result }}" == "success" ]; then
            echo "✅ All E2E tests passed!"
            exit 0
          else
            echo "❌ Some E2E tests failed"
            exit 1
          fi
